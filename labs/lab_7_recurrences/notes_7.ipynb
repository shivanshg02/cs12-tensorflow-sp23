{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning on sequences\n",
    "When performing a task on a sequence, _memory_ is often very important.\n",
    "To borrow an example from [Chris Olah's excellent blog post on RNNs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) (you should probably read this!), a neural network trained to predict the next word will have trouble filling in the blank in \"I grew up in France [ . . . ] I speak fluent  ____\" without memory.\n",
    "If it takes a fixed-size window as input, it may predict that the next word is the name of a language, but if the first sentence comes much earlier in the text, it would need to guess which language.\n",
    "\n",
    "Another problem is how to deal with variable-length sequences.\n",
    "1-D convolution is possible on a variable-length sequence, producing another variable-length sequence, but if we want to \"sum up\" an entire sequence as a single vector or number (for example, classifying a sentence by content), it's unclear how to do that.\n",
    "Similarly, for a fixed-size input, it's unclear how we could produce a variable-sized output.\n",
    "For instance, we might want to train a neural network to generate captions for images:\n",
    "\n",
    "![image captioning RNN](./images/captioning.png)\n",
    "(Image source: [\"Deep Visual-Semantic Alignments for Generating Image Descriptions\"](https://cs.stanford.edu/people/karpathy/deepimagesent/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple recurrent networks\n",
    "These two problems suggest that we'd like to have a specialized kind of neural network for sequences, one which learns to pick out parts of the input to remember later.\n",
    "This is the key idea behind a **recurrent neural network** (RNN), which maintains a **hidden state** that evolves as it reads through the input and governs how it produces output.\n",
    "\n",
    "At every timestep $t$, a simple RNN:\n",
    " 1. Reads an input $\\vec{x_t}$, and computes an activation based on the input $U \\vec{x_t}$ for a learned matrix $U$.\n",
    " 2. Reads the previous hidden state $\\vec{h_{t-1}}$, and computes an activation based on the previous hidden state $W \\vec{h_{t-1}}$ for a learned matrix $W$.\n",
    " 3. Computes a new hidden state for this timestep $\\vec{h_t}$ by applying a nonlinearity (usually tanh) to the sum of these two activations, and a learned bias term: $\\vec{h_t} = \\tanh \\big( \\vec{b} + W \\vec{h_{t-1}} + U \\vec{x_t} \\big )$\n",
    " 4. Computes an output for this timestep $\\vec{y_t}$ using the new hidden state: $\\vec{y_t} = \\theta(\\vec{c} + V \\vec{h_t})$ where $\\theta$ is an appropriate activation function and $\\vec{c}$ is a learned bias parameter\n",
    " \n",
    "(For this formulation, see the [Deep Learning Book, Chapter 10](http://www.deeplearningbook.org/contents/rnn.html))\n",
    "\n",
    "![RNN example](https://karpathy.github.io/assets/rnn/charseq.jpeg)\n",
    "(Image source: [\"The Unreasonable Effectiveness of Recurrent Neural Networks\"](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy, which you should definitely also read)\n",
    "\n",
    "Any neural network that operates over multiple timesteps, where the output or hidden state of one timestep is used as input for others, is called \"recurrent\".\n",
    "An RNN layer that updates a hidden state in exactly this way is called a \"simple RNN\" -- we will introduce more complicated RNN variants later. \n",
    "\n",
    "tanh is the activation function of choice for producing the hidden state of simple RNNs, since it has nice properties (linearity near zero, no exploding values, not setting values to zero like ReLU) when applied repeatedly in sequence,  which we do when processing with multiple timesteps.\n",
    "\n",
    "One view is that this forms a network with \"loops\", or _layers with self-connections_.\n",
    "These loops allow the hidden state in one timestep $t-1$ to influence the hidden state in the next timestep $t$, with additive influence from the input at time $t$ through one regular dense layer, and another regular dense layer producing the output at time $t$.\n",
    "\n",
    "Equivalently, RNNs can be viewed **unrolled**, where different timesteps are seen as different parts of the neural network.\n",
    "In this view, every connection in an RNN is an ordinary dense layer, with the exceptions that:\n",
    " - the hidden-to-hidden layers share weights\n",
    " - the input-to-hidden layers share weights\n",
    " - the hidden-to-output layers share weights\n",
    " - the new hidden state is based on the sum of the hidden-to-hidden layer and the input-to-hidden layer, passed through an activation function\n",
    "\n",
    "![unrolling RNNs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n",
    "(Image source: [\"Understanding LSTM Networks\"](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) on Colah's blog. In this image $h$ is the output and $A$ is the hidden state.)\n",
    "\n",
    "One neat interpretation of RNNs is that they learn to carry out an algorithm, which the elements of the hidden state acting as local variables.\n",
    "In this view:\n",
    " - The input-to-hidden matrix $U$ determines how new inputs influence the local variables $h$\n",
    " - The hidden-to-hidden matrix $W$ determines how the local variables should influence each other in one step of the algorithm\n",
    " - The hidden-to-output matrix $V$ determines how the local variables should combine to produce one step of output\n",
    " \n",
    "RNNs can be made deep (multi-layer) too, by adding a second recurrent layer that takes as its (variable-length) input the sequence of outputs of the first recurrent layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common structures for recurrent networks\n",
    "RNNs can use their hidden state as memory, learning what to incorporate from the input ($U$), what to remember ($W$), and how to produce output based on what's remembered ($V$).\n",
    "\n",
    "It turns out RNNs also solve our second problem with operating over sequences: they can handle arbitrary-length sequences as input or output.\n",
    "\n",
    "![RNN structures](https://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "(Image source: [\"The Unreasonable Effectiveness of Recurrent Neural Networks\"](https://karpathy.github.io/2015/05/21/rnn-effectiveness/))\n",
    "\n",
    "RNNs \"by default\" look at one input vector and produce one output vector per timestep.\n",
    "This results in a \"synchronized\" architecture (image 5 above), where every input is associated with an output that can also use information from previous timesteps.\n",
    "\n",
    "However, by only taking the output vector from the last timestep, an RNN can produce a fixed-length output from a variable-sized input (\"many to one\").\n",
    "Sentiment analysis is one example of this.\n",
    "When the output vector is treated as a representation to be input to other layers in the neural network, it's interpreted as \"summarizing\" the sequence into a fixed-size vector.\n",
    "\n",
    "Similarly, by only using the input vector for the first timestep, we can have an RNN produce variable-length output from a fixed-size input (\"one to many\").\n",
    "For instance, if the input is a high-level representation learned by a convolutional network on images, we could feed this (fixed-size) vector into an RNN to produce a description of the image.\n",
    "To know when to stop making new timesteps, a special \"end-of-sequence\" output is added to the end of every training example; when the network outputs this symbol, generation stops.\n",
    "\n",
    "Commonly, these two ideas are combined to form a \"sequence to sequence\" or \"encoder-decoder\" model (image 4 above) in which one RNN (the \"encoder\") summarizes a sequence into a fixed-length vector, and the a second RNN (the \"decoder\") takes the fixed-length vector and produces a variable-length output sequence.\n",
    "This flexible architecture enables translating between sequences of two different (arbitrary) lengths.\n",
    "One obvious application is translation, though this idea's seen use in lots of places (speech-to-text, text-to-speech, video captioning, ...).\n",
    "\n",
    "If you're curious, check out: [the paper that introduced the idea](https://arxiv.org/abs/1409.3215) and [the Keras blog's tutorial](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation through time\n",
    "RNNs are trained with a variant of backpropagation called \"backpropagation through time,\" which, for a given training sequence, computes the gradient updates to each of the weight matrices and bias term from each timestep $t = 1, \\ldots, T$ and sums them into a total update for that sequence. \n",
    "\n",
    "Equivalently, backpropagation through time is really just regular backpropagation applied to the unrolled computational graph.\n",
    "Because the parameters $U$, $V$, $W$, and $b$ are shared across each timestep, the total gradient updates for these parameters are summed across all timesteps.\n",
    "\n",
    "Because this unrolled computational graph is $T$ times larger than the computational graph with loops, training an RNN can often take much longer than training other kinds of networks.\n",
    "\n",
    "For a more complete coverage of the math behind backpropagation through time (though it is really about as simple as I've said here), see [WildML's tutorial](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/) (which uses mainly words and has some sample code) or [this full derivation](https://github.com/go2carter/nn-learn/blob/master/grad-deriv-tex/rnn-grad-deriv.pdf) (which goes into more depth and uses more math)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-term dependencies\n",
    "There's a huge problem with simple RNNs limiting their modeling power: they have a very hard time learning long-term dependencies (for example, learning how the first word in a sentence interacts with the last word).\n",
    "This is because the interaction between the input in timestep $t_1$, $x_{t_1}$, and the hidden state in timestep $t_2 >> t_1$, $h_{t_2}$, involves passing through many hidden states. \n",
    "When doing backpropagation through time, this means that these terms are the result of many matrix multiplications and passes through the tanh function.\n",
    "\n",
    "This results in two issues that prevent simple RNNs from learning how to relate pieces of information from very different timesteps to each other.\n",
    "\n",
    "### Exploding and vanishing gradients\n",
    "Ignoring the activation function for a moment (somewhat reasonable since the tanh function looks linear for inputs near zero), these longterm interactions look like raising the hidden-hidden matrix to a high power ($t_2 - t_1$) and multiplying it by the hidden state at time $t_1$, $h_{t_1}$.\n",
    "\n",
    "This is similar to the [power iteration algorithm in linear algebra](https://en.wikipedia.org/wiki/Power_iteration), with similar results: components of $h_{t_1}$ that are not aligned  with the largest eigenvector of $W$ decay to zero, and those that are explode to very high values.\n",
    "Both of these effects happen exponentially fast.\n",
    "\n",
    "Now considering the activation function again, the situation gets even worse.\n",
    "In these long-term interactions, the gradient is passed backwards through the function that results from composing the tanh activation function many times.\n",
    "A plot of this function applied to a high-dimensional hidden state reveals that it contributes to the problem of vanishing and exploding gradients (and looks very hard to optimize overall).\n",
    "\n",
    "![repeated application of tanh](./images/repeated_tanh.png)\n",
    "(Image source: the [Deep Learning Book, Chapter 10](http://www.deeplearningbook.org/contents/rnn.html))\n",
    "\n",
    "The net result of this is that while in theory simple RNNs are able to learn to use their hidden state to remember important information for long time periods, learning this behavior with gradient descent is extremely difficult. \n",
    "\n",
    "### Gradient clipping\n",
    "Exploding gradients are rare relative to vanishing gradients, but when they occur, gradient descent can cause the network parameters to make a large jump instead of a small step, and the optimizer needs to restart minimizing the loss from scratch.\n",
    "Luckily, this has a simple and effective fix: **gradient clipping**, in which gradient updates of above a certain magnitude are \"clipped\" down to that magnitude, preventing large parameter jumps.\n",
    "There are principled ways of picking the magnitude to clip at, but in practice 1 works well.\n",
    "\n",
    "Gradient clipping is essential for training RNNs, but it can be helpful in training other kinds of neural networks and doesn't really come with a cost.\n",
    "I use it often.\n",
    "\n",
    "However, because vanishing gradeints are extremely common in simple RNNs, even gradient clipping does not suffice to effectively train them.\n",
    "\n",
    "For more information on the problem of exploding and vanishing gradients, see section 10.7 in the [Deep Learning Book](http://www.deeplearningbook.org/contents/rnn.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient clipping in TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "tf_opt = tf.optimizers.SGD(1e-2)\n",
    "with tf.GradientTape() as g:\n",
    "    loss = _loss(target, actual)\n",
    "    \n",
    "grads = g.gradient(loss, model.trainable_variables)\n",
    "capped_grads = [tf.clip_by_norm(grad, clip_norm=1.) for grad in grads]\n",
    "\n",
    "tf_opt.apply_gradients(capped_grads, model.trainable_variables)\n",
    "\n",
    "# Gradient clipping in Keras\n",
    "import keras\n",
    "keras_opt = keras.optimizers.SGD(1e-2, clipnorm=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM recurrences\n",
    "One successful approach to solving the problem of vanishing gradients is \"gated recurrences\", of which LSTM (long short-term memory) cells are the most common.\n",
    "LSTM cells, in addition to a hidden state, propagate a **cell state** between time steps which helps the gradient backpropagate with high magnitude across many time steps.\n",
    "\n",
    "![LSTM cell](./images/lstm.png)\n",
    "(Image source: [\"Understanding LSTM Networks\"](https://colah.github.io/posts/2015-08-Understanding-LSTMs/))\n",
    "\n",
    "LSTMs are pretty complicated, but they're also essential for training RNNs with any kind of long-term memory.\n",
    "I defer to [\"Understanding LSTM Networks\" on Chris Olah's blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) for a full explanation of how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems for recurrences\n",
    "Despite LSTM networks trained with gradient clipping seeing lots of success on many kinds of sequence tasks (see [\"\n",
    "The Unreasonable Effectiveness of Recurrent Neural Networks\"](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) for many good examples), the modern understanding is that these networks still don't exhibit very long term memory.\n",
    "\n",
    "Many tasks requiring long-term memory are now performed with feedforward architectures like 1-D convolutional networks (for example, [WaveNet](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)) that use a fixed amount of context to do prediction.\n",
    "[Empirical evidence](https://arxiv.org/abs/1803.01271) suggests that this fixed-size window of context is, in many cases, larger than the number of timesteps that LSTMs can in practice learn to remember things for.\n",
    "\n",
    "See [this post by Berkely artificial intelligence research](https://bair.berkeley.edu/blog/2018/08/06/recurrent/) for full coverage of the topic.\n",
    "\n",
    "However, all is not lost for recurrences.\n",
    "They have still been successful in many tasks, including [some state of the art results](https://arxiv.org/pdf/1604.08772).\n",
    "Many exciting developments, like [Neural Turing Machines](https://distill.pub/2016/augmented-rnns/), are based on recurrent layers, and many researchers think that more [advanced forms of RNNs](https://distill.pub/2016/augmented-rnns/) will lead to major breakthroughs in machine learning.\n",
    "If nothing else, [they're really cool](https://distill.pub/2016/handwriting/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks in Keras\n",
    "In Keras, recurrent layers are accomplished with [subclasses of its RNN base layer](https://keras.io/layers/recurrent/).\n",
    "These are mainly:\n",
    " - `keras.layers.SimpleRNN` for a simple RNN layer (this is not often used)\n",
    " - `keras.layers.LSTM` for a layer using LSTM cells\n",
    " - `keras.layers.GRU` for a layer using gated recurrent units (GRUs), a modification of LSTM cells\n",
    " \n",
    "Each of these shares a common set of parameters (see [the API documentation](https://keras.io/layers/recurrent/) for a full list):\n",
    " - `units`, the number of units to use in this layer (its width, or number of outputs)\n",
    " - If `return_sequences` is `True`, then the layer will produce an output for every input (there will be a \"sequence length\" axis in the output tensor). Otherwise, just the final output is considered, so the layer produces a fixed-size output.\n",
    " - If `go_backwards` is `True`, the input is processed in reverse order.\n",
    "\n",
    "The input of an RNN layer has shape $(B, L, D)$ where $B$ is the batch size, $L$ is the sequence length (which can be `None` for variable-length sequences), and $D$ is the dimensionality of the input at each timestep.\n",
    " \n",
    "#### Masking\n",
    "To efficiently train RNNs, we often need to batch multiple training examples of the same length, which can mean needing padding values in the input.\n",
    "Keras enables \"masking\", which makes RNN layers ignore timesteps used for padding (they will not affect the hidden state during the forwards pass, or gradient calculations during the backwards pass).\n",
    "To do this, use an [Embedding](https://keras.io/layers/embeddings/) layer upstream and set `mask_zero` to `True` -- this will mask out 0 values in downstream RNN layers.\n",
    "\n",
    "#### Conditioning\n",
    "There are a number of ways to condition the output of an RNN on a value (for instance, to set which voice a text-to-speech system should use).\n",
    "One way is to pass the constant value as part of the input as every time step.\n",
    "To do this, use the `constants` keyword argument of a Keras RNN layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating TensorFlow and Keras\n",
    "Recall from earlier that you can use TensorFlow functionality to make custom Keras layers.\n",
    "The other direction is also possible: you can use Keras layers as part of a TensorFlow computational graph.\n",
    "This is an all-around excellent way to build models.\n",
    "It's especially useful for RNNs, since currently building them in pure TensorFlow is pretty awkward. We can extend this even further by writing our TensorFlow models as a subclass of `keras.Model`, meaning we can write fully custom Keras models. This gives us the ability to use Keras functions such as `model.fit()`.\n",
    "\n",
    "In the Keras functional API, which we introduced in lab 5, Keras layers are functions.\n",
    "Calling one of these functions on a TensorFlow tensor will return a new tensor representing its output.\n",
    "When you do this, Tensors, Variables, and Operations representing this layer are automatically added to the computational graph.\n",
    "Then, when you use an optimizer to minimize some loss, the variables used in this layer will also be modified.\n",
    "\n",
    "For a full guide to writing custom Keras models with TensorFlow, see [this article from the TensorFlow documentation](https://www.tensorflow.org/guide/keras/custom_layers_and_models).\n",
    "\n",
    "#### Generators\n",
    "As we'll see in the next example, for RNNs, our training data sequences can have variable lengths. This causes an issue when representing our data as a NumPy array, since the dimensions of axes must be equal. For example, if our training data looks like `[[1], [1, 2], [1, 2, 3]]`, we cannot convert it to a NumPy array. We could simply train our model on each sequence separately, but this would be clunky and defeat much of the purpose of Keras being a lightweight, easy-to-use framework. We can solve this problem by writing a generator function that yields batches to train on when called. We accomplish this by using the `yield` keyword in Python. You can think of this as like being able to return from a function without losing its internal state. For example,\n",
    "\n",
    "```\n",
    "def generator(n):\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        yield i\n",
    "        i += 1\n",
    "```\n",
    "\n",
    "When `generator(n)` is called, it returns an iterable of `[0, 1, ..., n]`. In other words, you can think of a generator as a Pythonic way of writing an iterator. Not only does this solve our problem of providing variable length batches to our model for training, it is crucial when we deal with giant datasets that cannot be stored in memory, as they are provided by the generator as we go, rather than cached in entirety. We can pass in a generator directly into the `model.fit()` function in Keras. Note that if our generator has a `while True` loop to infinitely generate values, we need to provide a `steps_per_epoch` argument to the `model.fit()` function so we know when we have completed one epoch.\n",
    "\n",
    "To learn more about Python generators, see this [article](https://realpython.com/introduction-to-python-generators/) and to read more about generators in Keras, see this [blog post](https://towardsdatascience.com/implementing-custom-data-generators-in-keras-de56f013581c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Toy RNN with TensorFlow and Keras\n",
    "In this example, we'll use Keras layers in a TensorFlow program to create a custom Keras model which trains a very small RNN to predict the next letter in the string \"hello\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "The data is structured as a list of (context, target) pairs, where the context is a variable-length list of letters and the target is a single letter.\n",
    "Each letter is one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# The string to replicate.\n",
    "# The ^ symbol acts as a \"start of string\" indicator.\n",
    "# The $ symbol acts as an \"end of string\" indicator.\n",
    "training_string = '^hello$'\n",
    "\n",
    "# All possible symbols\n",
    "dictionary = list(set(training_string))\n",
    "char_index = {}\n",
    "index_char = {}\n",
    "for i, char in enumerate(dictionary):\n",
    "    char_index[char] = i\n",
    "    index_char[i] = char\n",
    "\n",
    "n_symbols = len(dictionary)\n",
    "\n",
    "# Generator function that continually yields (context, next letter) pairs to train on\n",
    "def batch_generator(training_string):\n",
    "    training_ints = [char_index[char] for char in training_string]\n",
    "    training_onehot = tf.keras.utils.to_categorical(training_ints, n_symbols)\n",
    "    while True:\n",
    "        for i in range(1, len(training_onehot)):\n",
    "            context = training_onehot[:i]\n",
    "            next_letter = training_onehot[i]\n",
    "            yield np.expand_dims(context, axis=0), np.expand_dims(next_letter, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "The model uses a simple RNN layer with 2 units, followed by a softmax logistic regression, to predict the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality of the hidden and output spaces of the RNN\n",
    "n_recurrent_units = 2\n",
    "\n",
    "# Defining a keras model by subclassing `tf.keras.Model`\n",
    "class RNN(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, name=None, **kwargs):\n",
    "        super(RNN, self).__init__(name=name, **kwargs)\n",
    "        \n",
    "        self.recurrent_layer = tf.keras.layers.SimpleRNN(n_recurrent_units,\n",
    "                                                         return_state=True)\n",
    "        self.dense_layer = tf.keras.layers.Dense(n_symbols, activation='linear')\n",
    "        \n",
    "    def get_state(self, x):\n",
    "        # We can access the final hidden state as a tensor\n",
    "        _, state = self.recurrent_layer(x)\n",
    "        return state\n",
    "    \n",
    "    # Called when input is passed through the model\n",
    "    def call(self, x):\n",
    "        feats, state = self.recurrent_layer(x)\n",
    "        logits = self.dense_layer(feats)\n",
    "        return tf.nn.softmax(logits)\n",
    "    \n",
    "optimizer = tf.optimizers.RMSprop(2e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN()\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.9190\n",
      "Epoch 2/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.7272\n",
      "Epoch 3/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.6288\n",
      "Epoch 4/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.5484\n",
      "Epoch 5/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.4777\n",
      "Epoch 6/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.4156\n",
      "Epoch 7/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.3600\n",
      "Epoch 8/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.3080\n",
      "Epoch 9/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.2583\n",
      "Epoch 10/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.2102\n",
      "Epoch 11/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.1637\n",
      "Epoch 12/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.1190\n",
      "Epoch 13/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.0765\n",
      "Epoch 14/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.0365\n",
      "Epoch 15/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.9991\n",
      "Epoch 16/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9640\n",
      "Epoch 17/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9311\n",
      "Epoch 18/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.9001\n",
      "Epoch 19/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8708\n",
      "Epoch 20/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.8429\n",
      "Epoch 21/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.8163\n",
      "Epoch 22/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7908\n",
      "Epoch 23/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7663\n",
      "Epoch 24/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7429\n",
      "Epoch 25/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.7204\n",
      "Epoch 26/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.6988\n",
      "Epoch 27/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.6780\n",
      "Epoch 28/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6581\n",
      "Epoch 29/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.6391\n",
      "Epoch 30/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.6209\n",
      "Epoch 31/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.6035\n",
      "Epoch 32/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5869\n",
      "Epoch 33/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5710\n",
      "Epoch 34/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5558\n",
      "Epoch 35/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5413\n",
      "Epoch 36/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5275\n",
      "Epoch 37/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5142\n",
      "Epoch 38/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.5015\n",
      "Epoch 39/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4893\n",
      "Epoch 40/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4777\n",
      "Epoch 41/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4667\n",
      "Epoch 42/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.4579\n",
      "Epoch 43/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4454\n",
      "Epoch 44/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4355\n",
      "Epoch 45/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4277\n",
      "Epoch 46/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4160\n",
      "Epoch 47/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4078\n",
      "Epoch 48/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3985\n",
      "Epoch 49/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3896\n",
      "Epoch 50/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3827\n",
      "Epoch 51/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3728\n",
      "Epoch 52/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3681\n",
      "Epoch 53/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3562\n",
      "Epoch 54/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3515\n",
      "Epoch 55/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3417\n",
      "Epoch 56/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.3374\n",
      "Epoch 57/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3273\n",
      "Epoch 58/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3242\n",
      "Epoch 59/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3132\n",
      "Epoch 60/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3106\n",
      "Epoch 61/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3002\n",
      "Epoch 62/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.2979\n",
      "Epoch 63/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2876\n",
      "Epoch 64/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2859\n",
      "Epoch 65/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2756\n",
      "Epoch 66/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2742\n",
      "Epoch 67/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2641\n",
      "Epoch 68/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2630\n",
      "Epoch 69/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2531\n",
      "Epoch 70/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2522\n",
      "Epoch 71/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2427\n",
      "Epoch 72/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2419\n",
      "Epoch 73/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2327\n",
      "Epoch 74/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2319\n",
      "Epoch 75/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2233\n",
      "Epoch 76/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2223\n",
      "Epoch 77/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2143\n",
      "Epoch 78/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2131\n",
      "Epoch 79/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2057\n",
      "Epoch 80/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2043\n",
      "Epoch 81/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1976\n",
      "Epoch 82/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1958\n",
      "Epoch 83/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1899\n",
      "Epoch 84/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1876\n",
      "Epoch 85/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1826\n",
      "Epoch 86/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1797\n",
      "Epoch 87/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1757\n",
      "Epoch 88/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1722\n",
      "Epoch 89/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1691\n",
      "Epoch 90/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1649\n",
      "Epoch 91/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1628\n",
      "Epoch 92/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1580\n",
      "Epoch 93/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1569\n",
      "Epoch 94/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1514\n",
      "Epoch 95/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1511\n",
      "Epoch 96/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1451\n",
      "Epoch 97/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1457\n",
      "Epoch 98/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1391\n",
      "Epoch 99/100\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1404\n",
      "Epoch 100/100\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.1334\n",
      "WARNING:tensorflow:From /home/archi/Documents/courses/caltech-cs11-tensorflow-solutions/venv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./logs_lecture/model/assets\n"
     ]
    }
   ],
   "source": [
    "# Since our generator infinitely yields batches, we specify `steps_per_epoch`\n",
    "model.fit(batch_generator(training_string), steps_per_epoch=len(training_string) - 1, epochs=100)\n",
    "model.save('./logs_lecture/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output and hidden state visualization\n",
    "We run each sub-sequence through the model, observing that it successfully predicts the next character.\n",
    "This task requires memory, since the input string has a double letter in it.\n",
    "Then, we plot the two dimensions of the hidden state as a function of number of timesteps when reading the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: ['^']; Predicted char: h\n",
      "Context: ['^', 'h']; Predicted char: e\n",
      "Context: ['^', 'h', 'e']; Predicted char: l\n",
      "Context: ['^', 'h', 'e', 'l']; Predicted char: l\n",
      "Context: ['^', 'h', 'e', 'l', 'l']; Predicted char: o\n",
      "Context: ['^', 'h', 'e', 'l', 'l', 'o']; Predicted char: $\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "states = []\n",
    "\n",
    "i = 0\n",
    "for context, _ in batch_generator(training_string):\n",
    "    if i == len(training_string) - 1:\n",
    "        break\n",
    "    pred, st = model(context), model.get_state(context)\n",
    "    context_chars = [index_char[np.argmax(r)] for r in context[0]]\n",
    "    pred_char = index_char[np.argmax(pred)]\n",
    "    print('Context: {}; Predicted char: {}'.format(context_chars, pred_char))\n",
    "    states.append(st[0])\n",
    "    i += 1\n",
    "states = np.array(states)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(n_recurrent_units):\n",
    "    plt.plot(states[:, i])\n",
    "plt.xticks(list(range(len(training_string) - 1)),\n",
    "           training_string[:-1])\n",
    "plt.xlabel('Last input letter')\n",
    "plt.ylabel('Hidden activation')\n",
    "plt.title('Hidden state activation through time')\n",
    "plt.legend(list(range(1, n_recurrent_units + 1)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the hidden state\n",
    "The hidden state isn't always directly interpretable, but in this case both dimensions seem to track progress of the RNN through the string.\n",
    "Additionally, dimension 2 has low values near the beginning and end of the string, and may help the RNN know when to stop producing \"l\" characters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
