# Lab 7: Recurrent Neural Networks
This week we cover recurrent neural networks, which are specialized to operate over sequences.
Recurrent networks are the core of many modern models, including those used for translation, image captioning, and more advanced applications like learning to develop algorithms.
At the same time, most of the techniques required to make good recurrent models are complicated to implement, so we'll focus on concepts and high-level implementation using Keras.

Concepts:
 - Simple recurrent networks
 - Common formats for recurrent neural networks
 - Backpropagation through time
 - Vanishing and exploding gradients in simple recurrent networks
 - LSTM layers
 - Multi-layer recurrent networks

TensorFlow features:
 - Keras recurrent layer functions
 - Integrating TensorFlow and Keras

Optional resources:
 - ["The Unreasonable Effectiveness of Recurrent Neural Networks"](https://karpathy.github.io/2015/05/21/rnn-effectiveness/), by Andrej Karpathy, is an incredible resource for explaining both how RNNs work and some exciting implications
 - ["Understanding LSTMs"](https://colah.github.io/posts/2015-08-Understanding-LSTMs/), on Chris Olah's blog, is a great explanation of the complicated parts of LSTMs
 - [The Deep Learning Book chapter on recurrent networks](http://www.deeplearningbook.org/contents/rnn.html) is long but a definitive resource on this topic
 - [Attention and Augmented RNNs](https://distill.pub/2016/augmented-rnns/), posted on [distill.pub](https://distill.pub/), covers some more advanced topics of current research in RNNs
